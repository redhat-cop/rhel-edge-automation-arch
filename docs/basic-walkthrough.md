# Basic Walkthrough

This guide will help familiarize yourself with the process of building your first RHEL for Edge image using this architecture. By the end of this walk-through, you will:

* Understand the primary components in the architecture
* Build a RHEL for Edge image from a Blueprint
* Publish a Kickstart file referencing the previously built RHEL for Edge image

## Prerequisites

The following requirements must be satisfied prior to beginning the walk-through:

1. OpenShift CLI Tool
2. Tekton CLI Tool
3. An OpenShift cluster provisioned with the tooling associated in this repository
3. Access to the OpenShift cluster as a user with `cluster-admin` access.

## Use Case Overview

This walk-through will illustrate the ease of building, publishing and consuming RHEL for Edge content. For the sample use case, an edge node with the [IBM Developer Model Asset Exchange: Weather Forecaster](https://github.com/IBM/MAX-Weather-Forecaster) application running in a container will be built and deployed. This process consists of the following:

* Run a series of pipelines that: 
  + Use Image Builder to create a custom RHEL for Edge image (OSTree commit) using compose image type `rhel-edge-container`
  + Push generated OCI tar archive to Quay
  + Deploy OCI container on OpenShift for staging
  + Synchronize OStree content in web server running on OpenShift for production
* Creating a Kickstart file with the configuration to run the container workload
* Generate auto-booting RHEL for Edge installer ISO with embedded OSTree commit and Kickstart

## Building a RHEL for Edge Image

The process of building a RHEL for edge image involves composing a Blueprint containing a list of packages, entry modules for packages, as well as any customizations to the resulting image. The architecture includes a Tekton pipeline with the purpose of building an RHEL for Edge image from an existing blueprint. Sample blueprints are found on the [blueprints](https://github.com/redhat-cop/rhel-edge-automation-arch/tree/blueprints) branch of this repository.

For the most basic configurations, a sample [hello-world](https://github.com/redhat-cop/rhel-edge-automation-arch/tree/blueprints/hello-world) blueprint is available and provides the necessary configuration to run the containerized application.

All of the content for managing RHEL for Edge applications are located in the `rfe` namespace within the OpenShift cluster.

Log in to the OpenShift CLI and change into the `rfe` namespace:

```shell
oc project rfe
```

The `rfe-oci-image-pipeline` Tekton pipeline is responsible for building new RHEL for Edge images and storing the resulting OCI container with the OSTree Commit in Quay.

From the root of the project, execute the following command to instantiate the `rfe-oci-image-pipeline` to build the `hello-world` blueprint:

```shell
tkn pipeline start rfe-oci-image-pipeline \
--workspace name=shared-workspace,volumeClaimTemplateFile=openshift/resources/pipelines/volumeclaimtemplate.yaml \
-s rfe-automation \
--use-param-defaults \
-p blueprint-dir=hello-world 
```

To break down the preceding command:

* `tkn` - Tekton CLI
* `pipeline` - Resource to manage.
* `start` - Action to perform. Starts a pipeline run.
* `--workspace name=shared-workspace,volumeClaimTemplateFile=openshift/resources/pipelines/volumeclaimtemplate.yaml` - Specifies that a PersistentVolumeClaim should be used to back the Tekton workspace using a template found in the file [openshift/resources/pipelines/volumeclaimtemplate.yaml](https://github.com/redhat-cop/rhel-edge-automation-arch/blob/main/openshift/resources/pipelines/volumeclaimtemplate.yaml).
* `-s rfe-automation` - The name of the Service Account used to run the pipeline.
* `--use-param-default` - The default Pipeline parameters will be applied unless explicitly specified.
* `-p blueprint-dir=hello-world` - The directory containing the blueprint file in the cloned repository. By default, the _blueprints_ branch of this repository will be used.

The output of the command will provide a command to view the progress of the build.

_Note: The process of building a RHEL for Edge image takes time!_

### Verification

Once the pipeline completes, the OCI container generated by Image Builder should be stored in Quay. To verify, obtain the route to Quay by running the following command:

```shell
oc get quayregistry quay -n quay -ojsonpath='{.status.registryEndpoint}'
```

Quay is not setup to use OpenShift's SSO, so the username can be found by running:

```shell
oc get secret quay-rfe-setup -n rfe -o go-template='{{ .data.username | base64decode }}'
```

And the password by running:

```shell
oc get secret quay-rfe-setup -n rfe -o go-template='{{ .data.password | base64decode }}'
```

Once logged in, click on the RFE organization to the right of the page under _Users and Organizations_ and then select the repository name associated with your blueprint.

To the left of the screen, click the _Tags_ icon to view associated tags. Each pipeline run will create two tags:

* A _latest_ tag that points to the most recent image.
* A tag with the version specified in the blueprint.

At this point, you could manually pull/deploy the container for use in the deployment of RFE content.

### Pipeline Results

Each pipeline run returns three results:

* `build-commit` - The Build Commit ID from Image Builder
* `image-path` - Location in Quay registry of OCI container
* `image-tags` - Tags applied to the container (JSON list)

To view the results, find the latest pipeline run. Use the following command as an example:

```shell
$ tkn pipelinerun list -n rfe --label tekton.dev/pipeline=rfe-oci-image-pipeline --limit 1
NAME                               STARTED     DURATION     STATUS
rfe-oci-image-pipeline-run-2lpwc   1 day ago   13 minutes   Succeeded
```

Then run the following to view the pipeline results:

```shell
$ oc get pipelinerun rfe-oci-image-pipeline-run-2lpwc -ojsonpath='{.status.pipelineResults}' | jq .
[
  {
    "name": "build-commit",
    "value": "ab07f144-43a7-49b3-93de-99e1562435f9"
  },
  {
    "name": "image-path",
    "value": "quay-quay-quay.apps.cluster.com/rfe/hello-world"
  },
  {
    "name": "image-tags",
    "value": "[\"latest\", \"0.0.1\"]"
  }
]
```

## Staging OCI Container with OSTree Commit

Now that we have our OCI container from Image Builder with our OSTree Commit, we can run a pipeline to deploy it as a staging environment in OpenShift.

From the root of the project, execute the following command to instantiate the `rfe-oci-stage-pipeline` to deploy the OCI container built in the previous pipeline (`rfe-oci-image-pipeline`) run.

```shell
tkn pipeline start rfe-oci-stage-pipeline \
--workspace name=shared-workspace,volumeClaimTemplateFile=openshift/resources/pipelines/volumeclaimtemplate.yaml \
-s rfe-automation \
--use-param-defaults \
-p image-path=$(oc get quayregistry quay -n quay -ojsonpath='{.status.registryEndpoint}')/rfe/hello-world \
-p image-tag=latest
```

This command is similar to the previous pipeline run, but the following parameters are used:

* `-p image-path=...` - The path to the OCI container stored in the Quay registry.
* `-p image-tag=latest` - Use the image with the tag _latest_.

### Verification

Once the pipeline runs, an ImageStream, Deployment, Service and Route are configured in the `rfe` namespace.

### Pipeline Results

Each pipeline run returns one result:

* `content-path` - The path to the OSTree repository.

To view the results, find the latest pipeline run. Use the following command as an example:

```shell
$ kn pipelinerun list --label tekton.dev/pipeline=rfe-oci-stage-pipeline --limit 1
NAME                               STARTED     DURATION     STATUS
rfe-oci-stage-pipeline-run-cxkxq   1 day ago   13 minutes   Succeeded
```

Then run the following to view the pipeline results:

```shell
$ oc get pipelinerun -n rfe rfe-oci-stage-pipeline-run-cxkxq -ojsonpath='{.status.pipelineResults}' | jq .
[
  {
    "name": "content-path",
    "value": "http://hello-world-latest-rfe.apps.cluster.com/repo"
  }
]
```

## Creating the Kickstart File

A Tekton pipeline similar to the `rfe-tarball-pipeline` pipeline called `rfe-kickstart-pipeline` is responsible for publishing a Kickstart file to both Nexus and the HTTPD server. As the pipeline uses Ansible, Jinja based templating is available to inject key values (in particular, the location of the extracted rpm-ostree tarball in the HTTPD server).

Using the location of the extracted RHEL for Edge image produced by the pipeline in the previous section, execute the following command from the root of the repository to start the `rfe-kickstart-pipeline` pipeline.

```shell
tkn pipeline start rfe-kickstart-pipeline --workspace name=shared-workspace,volumeClaimTemplateFile=openshift/resources/pipelines/volumeclaimtemplate.yaml --use-param-defaults -p kickstart-path=ibm-weather-forecaster/kickstart.ks -s rfe-automation -p rfe-tarball-url=$(oc get pipelinerun $(oc get pipelinerun -l=tekton.dev/pipeline=rfe-tarball-pipeline --sort-by=".status.completionTime" -o jsonpath='{ .items[-1].metadata.name }') -o jsonpath='{ .status.pipelineResults[?(@.name=="serving-storage-url")].value }')
```

To break down the preceding command:

1. `tkn` - Tekton CLI
2. `pipeline` - Resource to manage
3. `start` - Action to perform. Starts a pipeline
4. `--workspace name=shared-workspace,volumeClaimTemplateFile=openshift/resources/pipelines/volumeclaimtemplate.yaml` - Specifies that a PersistentVolumeClaim should be used to back the Tekton workspace using a template found in the file [openshift/resources/pipelines/volumeclaimtemplate.yaml](openshift/resources/pipelines/volumeclaimtemplate.yaml).
5. `--use-param-default` - The default Pipeline parameters will be applied unless explicitly specified
6. `-p ibm-weather-forecaster/kickstart.ks` - The location of the kickstart to use in the referenced repository. By default, the _kickstarts_ branch of this repository will be used
7. `-s rfe-automation` - THe name of the Service Account to run the pipeline as
8. `rfe-tarball-url=<URL_FROM_PRIOR_PIPELINE>` - The location of the kickstart to use in the referenced repository. By default, the _kickstarts_ branch of this repository will be used

The output of the command will provide a command to view the progress of the build.

### Verification

Once the pipeline completes, the assets can then be verified in both Nexus and HTTPD.

First, obtain the URL of the Nexus Route and login with your OpenShift credentials

```shell
oc get routes -n rfe nexus -o jsonpath=http://'{ .spec.host }'
```

Once logged in, select the *Browse* button on the left hand navigation bar and then select the *rfe-kickstarts* repository.

The list of uploaded kickstarts separated by directory will be displayed.

To verify the kickstart in the HTTPD server, first execute the following command to obtain a remote shell session on the server:

```shell
oc rsh -n rfe $(oc get pods -l=deployment=httpd -o jsonpath='{.items[0].metadata.name }')
```

Display the content of the kickstart file

```shell
cat /opt/rh/httpd24/root/var/www/html/kickstarts/ibm-weather-forecaster/kickstart.ks
exit
```

The PipelineRun resource also provides the externally facing location for these assets in both Nexus and HTTPD as execution results. The most important resource in this section is the location of the uploaded kickstart file which can be found by executing the following command:

```shell
oc get pipelinerun $(oc get pipelinerun -l=tekton.dev/pipeline=rfe-kickstart-pipeline --sort-by=".status.completionTime" -o jsonpath='{ .items[-1].metadata.name }') -o jsonpath='{ .status.pipelineResults[?(@.name=="serving-storage-url")].value }'
```

## Creating a RHEL for Edge Node

Now that the the Kickstart and the contents of the rpm-ostree tarball are available in the HTTPD server, create a new RHEL for Edge Node by booting a new machine using the RHEL 8 boot image.

At the boot menu, hit the tab key and add the following to the list of boot arguments:

```shell
inst.ks=<URL_OF_KICKSTART_FILE>
```

Hit enter to boot the machine using the Kickstart. The machine will retrieve the rpm-ostree content and prepare the machine to run the sample application. Once complete, the machine will reboot

### Verify the application

Once the machine has been rebooted, login as the user created as part of the node installation.

_Note: By default, this example specifies the following `core` as the user and `edge` as the password.

Once logged in, confirm the application container is running:

```shell
sudo podman ps
```

Finally, confirm that the application Swagger endpoint responds to requests:

```shell
curl localhost:5000
```

Additional details on interacting with the application can be found in the [project repository](https://github.com/IBM/MAX-Weather-Forecaster)

You have now successfully completed the walkthrough!
